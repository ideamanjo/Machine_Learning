{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_basic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Cloud for Big Data",
      "language": "python",
      "name": "bdc"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ideamanjo/Machine_Learning/blob/master/TF_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UwlPAA8ZJUsr"
      },
      "source": [
        "Copyright (C) 2019 Software Platform Lab, Seoul National University\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); \n",
        "\n",
        "you may not use this file except in compliance with the License. \n",
        "\n",
        "You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 \n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software \n",
        "\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS, \n",
        "\n",
        "\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n",
        "\n",
        "\n",
        "See the License for the specific language governing permissions and\n",
        "\n",
        "\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "02uuobu-dalC",
        "colab": {},
        "outputId": "68db780f-7ba1-474b-801f-e6be79e0e034"
      },
      "source": [
        "%env CUDA_VISIBLE_DEVICES= 3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ki_RHIwPJvyn"
      },
      "source": [
        "# 1. TensorFlow Ops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s97fwEOoiXk3"
      },
      "source": [
        "## Graph and Session\n",
        "**Graph** : It contains a set of Operations (units of computation) and Tensors (data flow between operations).\n",
        "\n",
        "**Session** : It encapsulates an execution environment such as which operations are executed and what is the current values of Tensor objects. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8p7tgEPijeSd"
      },
      "source": [
        "Let's learn Graph and Session using examples presented below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V5N1npMVQqJz"
      },
      "source": [
        "## Constant Op\n",
        "\n",
        "Let's create a constant in TensorFlow.\n",
        "\n",
        "**```tf.constant(value, dtype = None, shape = None, name = 'Const', verify_shape = False)```**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EFZ3bfVsQz_p",
        "colab": {},
        "outputId": "53a01ee2-ef54-4ce6-d61e-4d861f7674e9"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # constant of 1d tensor, or a vector\n",
        "  a = tf.constant([2,2], name = 'vector')\n",
        "\n",
        "  # constant of 2x2 tensor, or a matrix\n",
        "  b = tf.constant([[0,2], [1,3]], name = 'matrix')\n",
        "\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"vector:0\", shape=(2,), dtype=int32)\n",
            "Tensor(\"matrix:0\", shape=(2, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tqBA4_VcXCBe",
        "colab": {},
        "outputId": "d3215e36-c099-4079-b7cc-7658c086d1b8"
      },
      "source": [
        "# Get values of a and b\n",
        "with tf.Session(graph=graph) as sess:\n",
        "  print('a: ')\n",
        "  print(sess.run(a))\n",
        "  print('\\nb:')\n",
        "  print(sess.run(b))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a: \n",
            "[2 2]\n",
            "\n",
            "b:\n",
            "[[0 2]\n",
            " [1 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xrE4WkZNUn9o"
      },
      "source": [
        "## Math Ops\n",
        "The following example shows a matrix division op."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "drceRvn4VGec",
        "colab": {},
        "outputId": "c6067772-b1a0-4f95-aba5-f4db3159c4d2"
      },
      "source": [
        "graph = tf.Graph() #앞에서 사용한 그래프와 같은 중복이 되는 상황 발생되어 명시적으로 구분\n",
        "with graph.as_default():\n",
        "  # Create constant a and b\n",
        "  a = tf.constant([2,4], name = 'a', dtype = tf.float32)\n",
        "  b = tf.constant([[0,1], [2,3]], name = 'b', dtype = tf.float32)\n",
        "  \n",
        "  # Create divide operation using b and a\n",
        "  div = tf.div(b, a)\n",
        "  # or equivalently, div = b / a\n",
        "  # div = b / a\n",
        "\n",
        "print('Print information of div op')\n",
        "print(div.op)\n",
        "\n",
        "print('\\nPrint div tensor')\n",
        "print(div)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-f52495e6d082>:8: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Print information of div op\n",
            "name: \"div\"\n",
            "op: \"RealDiv\"\n",
            "input: \"b\"\n",
            "input: \"a\"\n",
            "attr {\n",
            "  key: \"T\"\n",
            "  value {\n",
            "    type: DT_FLOAT\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "Print div tensor\n",
            "Tensor(\"div:0\", shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jthhyswKkisO"
      },
      "source": [
        "Run div operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kkmWJN_pY8Xf",
        "colab": {},
        "outputId": "134ef889-5502-462e-dbe3-14269fa94336"
      },
      "source": [
        "with tf.Session(graph=graph) as sess:\n",
        "  print('\\nPrint div')\n",
        "  print(sess.run(div))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Print div\n",
            "[[0.   0.25]\n",
            " [1.   0.75]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cnNhG69dgWsA"
      },
      "source": [
        "## Quiz 1\n",
        "**Define a graph with two constants with shape=[2, 2] and a matmul operation. Print the values of c.\n",
        "(X: matrix multiplication, HINT: use tf.matmul) **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QZyw665-gWsE",
        "colab": {},
        "outputId": "842ce7c0-4518-4cec-da9c-46d50e984856"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ############# Write here. ################\n",
        "    a = tf.constant([[2,4],[1,2]], name = 'a', dtype = tf.float32)\n",
        "    b = tf.constant([[0,1], [2,3]], name = 'b', dtype = tf.float32)\n",
        "    c = tf.matmul(b, a)\n",
        "\n",
        "  ##########################################\n",
        "\n",
        "with tf.Session(graph=graph) as sess:\n",
        "  print(sess.run(c))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.  2.]\n",
            " [ 7. 14.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a4UKKwwsZa4I"
      },
      "source": [
        "## Variables\n",
        "\n",
        "TensorFlow object to store mutable state (e.g., model parameters) across multiple session runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2TubsFPeZ0Rz"
      },
      "source": [
        "### Creating variables\n",
        "\n",
        "To declare a variable, you create an instance of the class tf.Variable. tf.constant is written as lowercase because it's an op, and tf.Variable is written with a capital \"V\" because it encapsulates multiple ops.\n",
        "\n",
        "#### Usage of TF Variables\n",
        "\n",
        "\n",
        "```\n",
        "x = tf.Variable(...)\n",
        "x               # variable op\n",
        "x.initializer   # initialization ops\n",
        "x.value         # read op\n",
        "x.assign(...)   # write op\n",
        "x.assign_add(...) # x += ...\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zcriAnGbaCKk"
      },
      "source": [
        "One way to create a variable is: \n",
        "\n",
        "**```tf.Variable(< initial-value >, name = < optional-name >)```**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DsaTk0M3hr0p"
      },
      "source": [
        "This example creates three variables using `tf.Variable`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "32t-sxgsaFXh",
        "colab": {}
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # Create scalar variable\n",
        "  s = tf.Variable(2, name = 'scalar') #name은 python코드의 variable같은 개념 operation name으로 별 문제 없다. name이 없으면 default name으로 정해짐\n",
        "  # Create matrix variable\n",
        "  m = tf.Variable([[0,1], [2,3]], name = 'matrix')\n",
        "  # Create zero matrix using tf.zeros\n",
        "  W = tf.Variable(tf.zeros([784,10]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O91x5GeZzW2N",
        "colab": {},
        "outputId": "e2e4bf98-a298-4a8a-b7b7-db2b77460951"
      },
      "source": [
        "with tf.Session(graph=graph) as sess:\n",
        "  print('s:')\n",
        "  print(sess.run(s))\n",
        "  print('\\nm:')\n",
        "  print(sess.run(m))\n",
        "  print('\\nW:')\n",
        "  print(sess.run(W))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "Attempting to use uninitialized value scalar\n\t [[{{node _retval_scalar_0_0}}]]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/home/yunseonglee/venv-bdc/lib/python3.5/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/yunseonglee/venv-bdc/lib/python3.5/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/yunseonglee/venv-bdc/lib/python3.5/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value scalar\n\t [[{{node _retval_scalar_0_0}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-cab24378dbe0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nm:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/yunseonglee/venv-bdc/lib/python3.5/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/yunseonglee/venv-bdc/lib/python3.5/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/yunseonglee/venv-bdc/lib/python3.5/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/yunseonglee/venv-bdc/lib/python3.5/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value scalar\n\t [[{{node _retval_scalar_0_0}}]]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r9Xymzuia0j3"
      },
      "source": [
        "### Initialize variables\n",
        "\n",
        "Before using a variable, you must initialize it, or else you'll run into an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZpN5DVYmR-U",
        "colab": {}
      },
      "source": [
        "with tf.Session(graph=graph) as sess:\n",
        "  # Run initialization op\n",
        "  sess.run(s.initializer)\n",
        "  sess.run(m.initializer)\n",
        "  sess.run(W.initializer)\n",
        "\n",
        "  print('s:')\n",
        "  print(sess.run(s))\n",
        "  print('\\nm:')\n",
        "  print(sess.run(m))\n",
        "  print('\\nW:')\n",
        "  print(sess.run(W))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qr8gfZOqmSlC"
      },
      "source": [
        "To initiliaze all variables at once: use **`tf.global_variables_initializer()`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IFFDV5ujbjX9",
        "colab": {},
        "outputId": "0ed93ee1-0935-4fbb-e3f7-90a01aad9a23"
      },
      "source": [
        "with tf.Session(graph=graph) as sess:\n",
        "  # Run initialization ops of all variables\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  print('s:')\n",
        "  print(sess.run(s))\n",
        "  print('\\nm:')\n",
        "  print(sess.run(m))\n",
        "  print('\\nW:')\n",
        "  print(sess.run(W))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s:\n",
            "2\n",
            "\n",
            "m:\n",
            "[[0 1]\n",
            " [2 3]]\n",
            "\n",
            "W:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Us76wF-aTaf"
      },
      "source": [
        "More encouraged way is using **`tf.get_variable`**, which allows us to provide the variable's internal name, shape, type, and initializer to give the variable its initial value.\n",
        "\n",
        "```\n",
        "tf.get_variable(\n",
        "    name,\n",
        "    shape=None,\n",
        "    dtype=None,\n",
        "    initializer=None,\n",
        "    regularizer=None,\n",
        "    trainable=True,\n",
        "    collections=None,\n",
        "    caching_device=None,\n",
        "    partitioner=None,\n",
        "    validate_shape=True,\n",
        "    use_resource=None,\n",
        "    custom_getter=None,\n",
        "    constraint=None\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "54ABBw1ljKqB"
      },
      "source": [
        "Create three variables using `tf.get_variable`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TV2kmEqUaeF8",
        "colab": {}
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  s = tf.get_variable('scalar', initializer=tf.constant(3))\n",
        "  m = tf.get_variable('matrix', initializer=tf.constant([[0,2], [1,3]]))\n",
        "  W = tf.get_variable('big_matrix', shape=(784, 10), initializer=tf.ones_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9iVhrjR-aptK",
        "colab": {},
        "outputId": "679ff9e6-318a-46d4-d327-c3d0f160fc17"
      },
      "source": [
        "with tf.Session(graph=graph) as sess:\n",
        "  # Run initialization ops of all variables\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  print('s:')\n",
        "  print(sess.run(s))\n",
        "  print('\\nm:')\n",
        "  print(sess.run(m))\n",
        "  print('\\nW:')\n",
        "  print(sess.run(W))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s:\n",
            "2\n",
            "\n",
            "m:\n",
            "[[0 1]\n",
            " [2 3]]\n",
            "\n",
            "W:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "239nTB8ScEMD"
      },
      "source": [
        "### Changing values of variables\n",
        "\n",
        "To change the value of a variable, we need to assign a new value to the variable.\n",
        "You can see variable `v` changes after `assign` operations are executed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ezu6kFnScVd8",
        "colab": {},
        "outputId": "21469745-0191-4d63-c643-8753f29179b3"
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # v is a 2 x 3 variable of random values\n",
        "  v = tf.get_variable('normal_matrix', shape=(2,3), initializer=tf.random_normal_initializer(mean=0., stddev=1.))\n",
        "  c = tf.constant(1.0, shape=(2,3))\n",
        "  assign_1 = v.assign(c)\n",
        "  assign_2 = v.assign([[1., 2., 3.], [4., 5., 6.]])\n",
        "\n",
        "\n",
        "with tf.Session(graph=graph) as sess:\n",
        "  # Initialize variables\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  # or equivalently\n",
        "  # sess.run(v.initializer)\n",
        "  \n",
        "  # Get value\n",
        "  print('v:')\n",
        "  print(sess.run(v))\n",
        "\n",
        "  # Assign new value to the variable\n",
        "  sess.run(assign_1)\n",
        "\n",
        "  # Get value again\n",
        "  print('v:')\n",
        "  print(sess.run(v))\n",
        "\n",
        "  # Initialize the variable once again to random values\n",
        "  sess.run(v.initializer)\n",
        "\n",
        "  # Get value again\n",
        "  print('v:')\n",
        "  print(sess.run(v))\n",
        "\n",
        "  # Assign new value to the variable\n",
        "  sess.run(assign_2)\n",
        "\n",
        "  # Get value again\n",
        "  print('v:')\n",
        "  print(sess.run(v))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "v:\n",
            "[[-1.8030622  -0.30408686 -1.7943084 ]\n",
            " [-1.7285426   0.39448568 -2.0410297 ]]\n",
            "v:\n",
            "[[1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "v:\n",
            "[[-0.2661235   0.86212033 -0.919069  ]\n",
            " [ 1.5030576  -0.34156126  0.18998317]]\n",
            "v:\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gs3yizqwmxHs"
      },
      "source": [
        "## Quiz 2\n",
        "Define a variable (name : \"term\") with shape = [] and dtype = `tf.float64`. Initialize the variable as `2` first.\n",
        "Define another variable (name : \"sum\") with shape = [] and dtype = `tf.float64`. Initialize the variable as zeros.\n",
        "\n",
        "By using these two variables, compute the following:\n",
        "sum = 1/term_1 + 1/term_2 + ... + 1/term_10\n",
        "where\n",
        "term_i = term_{i-1} * (term_{i-1} - 1) + 1\n",
        "and term_1 = 2.\n",
        "(This recurrence relation is known as Sylvester's sequence.)\n",
        "\n",
        "Hint: Repeat updating the variables \"sum\" and \"term\" 10 times.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rhqP7g40o0z2",
        "colab": {},
        "outputId": "8013c17b-cbfe-427e-ea26-7526f48ca1d6"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ############# Write here. ################\n",
        "  t = tf.get_variable('term', dtype=tf.float64, initializer=tf.constant(2., dtype=tf.float64))\n",
        "  s = tf.get_variable('sum', dtype=tf.float64, initializer=tf.constant(0., dtype=tf.float64))\n",
        "  update_t = t.assign(t * (t-1)+1)\n",
        "  update_s = s.assign_add(1/t)\n",
        "#assing += 랑 같다\n",
        "\n",
        "  ##########################################\n",
        "\n",
        "with tf.Session(graph=graph) as sess:\n",
        "  ############# Write here. ################\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for _ in range(10):\n",
        "    sess.run(update_s)\n",
        "    sess.run(update_t)\n",
        "  print('s:', sess.run(s))\n",
        "\n",
        "\n",
        "\n",
        "  ##########################################\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s: 0.9999999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rgFhwdh4kJlB"
      },
      "source": [
        "# 2. Feeding Input Data into a TensorFlow Graph\n",
        "\n",
        "How to feed data into a TensorFlow program?\n",
        "\n",
        "Use `tf.Placeholder` or\n",
        "\n",
        "**TensorFlow Dataset API (recommended for large-scale data)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JNTEqevyzz2X"
      },
      "source": [
        "## Placeholder\n",
        "\n",
        "TensorFlow's feed mechanism lets you inject data into any Tensor in a computation graph. Feed your data (usually a Numpy array) by passing the `feed_dict` argument to the `sess.run()` call.\n",
        "\n",
        "TensorFlow provides a **placeholder** operation that must be fed with data on execution. A placeholder exists solely to serve as the target of feeds. It is not initialized and contains no data. A placeholder generates an error if it is executed without a feed, so you won't forget to feed it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7l66JRS_03Xc",
        "colab": {},
        "outputId": "8ad75d80-9379-4ed6-821d-6ad49cc88b05"
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # Create a constant with 1.\n",
        "  a = tf.constant(1)\n",
        "\n",
        "  # Create scalar placeholder with integer data type.(이렇게 안하면 에러남)\n",
        "  b = tf.placeholder(shape=[], dtype=tf.int32)\n",
        "\n",
        "  # Create add operation using the constant and placeholder.\n",
        "  c = tf.add(a, b)\n",
        "  \n",
        "  with tf.Session(graph=graph) as sess:\n",
        "    print('a:', sess.run(a))\n",
        "\n",
        "    # cannot run the graph since we didn't feed value to the placeholder\n",
        "    try:\n",
        "      print('b:', sess.run(b))\n",
        "    except Exception as e:\n",
        "      print(e.message)\n",
        "\n",
        "    # cannot run the graph since we didn't feed value to the placeholder\n",
        "    try:\n",
        "      print('c:', sess.run(c))\n",
        "    except Exception as e:\n",
        "      print(e.message)\n",
        "\n",
        "    # feed value 2 to b (c = a + b = 1 + 2 = 3)\n",
        "    print('c = 1 + 2 =', sess.run(c, feed_dict={b: 2})) #key는 tensor, value는 값\n",
        "\n",
        "    # we can also feed a\n",
        "    print('c = -1 + 2 =', sess.run(c, feed_dict={a: -1, b: 2}))\n",
        "\n",
        "    # we can also feed c: in this case, we don't have to feed b since we don't need it\n",
        "    print('c =', sess.run(c, feed_dict={c:0}))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a: 1\n",
            "You must feed a value for placeholder tensor 'Placeholder' with dtype int32\n",
            "\t [[node Placeholder (defined at /home/yunseonglee/venv-bdc/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n",
            "You must feed a value for placeholder tensor 'Placeholder' with dtype int32\n",
            "\t [[node Placeholder (defined at /home/yunseonglee/venv-bdc/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n",
            "c = 1 + 2 = 3\n",
            "c = -1 + 2 = 1\n",
            "c = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lJ3XeX_Gn4a3"
      },
      "source": [
        "## Quiz 3\n",
        "Define a variable (name: `sum`, shape: [2, 3]).\n",
        "Initialize the variable as zeros first.\n",
        "\n",
        "Drawing random samples from a uniform distribution using `np.random.uniform(size=(2,3))`.\n",
        "\n",
        "Accumulate the value of the sample to the variable 1200 times.\n",
        "Subtract `600.0` from the variable and divide the variable by `10.0` to approximate a draw from standard normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N6VmIhrin4a4",
        "colab": {},
        "outputId": "5537fcd6-3be5-4fc9-c5ae-326fba685afe"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ############# Write here. ################\n",
        "    sample = tf.placeholder(shape=(2,3), dtype=tf.float32)\n",
        "    s = tf.get_variable('sum', shape=(2,3), initializer=tf.zeros_initializer())    \n",
        "    update_s = s.assign_add(sample)\n",
        "    normalized = (s - 600.)/10.\n",
        "\n",
        "  ##########################################\n",
        "\n",
        "with tf.Session(graph=graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    ############# Write here. ################\n",
        "    for _ in range(1200):\n",
        "        sess.run(update_s,feed_dict({sample: np.random.uniform(size=(2,3))}))\n",
        "\n",
        "    print(sses.run(normalized)  )\n",
        "\n",
        "\n",
        "  ##########################################"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'feed_dict' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-3d4b661bb6f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m############# Write here. ################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'feed_dict' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9mHYbZmS06zb"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The `tf.data` API is the most advanced API for writing TensorFlow input pipelines.\n",
        "\n",
        "It allows you to build complex pipelines by composing simple building blocks. \n",
        "\n",
        "Two main abstractions introduced by the `tf.data` API are:\n",
        "* `tf.data.Dataset`: contains a sequence of items (each item represents one or more `tf.Tensor`s)\n",
        "* `tf.data.Iterator`: provides interface to iterate through the dataset\n",
        "\n",
        "Users can create new Datasets from existing `tf.Tensor`s by using static methods like `Dataset.from_tensor_slices()`. \n",
        "\n",
        "For example, you can create a Dataset of string Tensors that represents input file names. \n",
        "\n",
        "Transformation of exisiting Datasets is another way of creating new dataset. \n",
        "\n",
        "TensorFlow provides frequently-used Dataset transformations such as `Dataset.batch` or `Dataset.shuffle` (please refer to https://www.tensorflow.org/api_docs/python/tf/data/Dataset). \n",
        "\n",
        "An Iterator is associated with a particular Dataset and we can retrieve the next element by executing an operation returned by `Iterator.get_next()`. \n",
        "\n",
        "This typically acts as an interface between your Dataset input pipline and your model.\n",
        "\n",
        "The simplest way to construct an Iterator is using `tf.compat.v1.data.make_one_shot_iterator(Dataset)`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G2N-qVWB4VNW"
      },
      "source": [
        "### `tf.data.Dataset.from_tensor_slices()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "knXKv93E4drK"
      },
      "source": [
        "Creates a Dataset whose elements are slices of the given *python array* or *numpy array* or *tensors*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7lVbbwCfzGbx",
        "colab": {},
        "outputId": "e99da6f3-9fa3-40c2-efe1-a53b0bcb4370"
      },
      "source": [
        "import numpy as np\n",
        "arr = np.arange(10) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # Create a dataset from a numpy array\n",
        "  ds = tf.data.Dataset.from_tensor_slices(arr)\n",
        "  \n",
        "  # Create an iterator for the dataset. 메이져버전 2.0이 몇주전 나옴. 약간 지저분해짐\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  \n",
        "  # Retrieve next element from theiterator\n",
        "  data_getter = iterator.get_next()\n",
        "  \n",
        "  # Multiply by 2\n",
        "  mul_2 = data_getter * 2\n",
        "  \n",
        "  with tf.Session(graph=graph) as sess:\n",
        "    for i in range(10):\n",
        "      print(sess.run(mul_2))\n",
        "    \n",
        "#place holder : 일종의 주머니 feeddict로 채워준다.\n",
        "#우리가 원하는대로 변환시키고, tf.place_holder를 값을 로드하고 그래프 실행하고, 디스크 로딩하는시간 gpu도는 시간이 따로라 원하는 성능안나옴\n",
        "#api data set 기본적으로 값들읽어오고 preprocessing도 해주고, 내부적으로 gpu실행 동시에 디스크로드해서 실행끝나고 바로 시작할수있는\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "10\n",
            "12\n",
            "14\n",
            "16\n",
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hV-LVSuslOLE"
      },
      "source": [
        "## Create a dataset from files using the Dataset API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z_ifr21zdal4",
        "colab": {}
      },
      "source": [
        "def iterate_and_print(iterator, graph, count=6):\n",
        "  with tf.Session(graph=graph) as sess:\n",
        "    # Read the first `count` elements of the iterator.\n",
        "    for i in range(count):\n",
        "      v = sess.run(iterator)\n",
        "      print('step %d, data: %s' % (i, v))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mCt7x4ByogNm"
      },
      "source": [
        "### Create dummy binary files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MTFhI2kBomkO",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def create_bin_file(file_name, value):\n",
        "  with open(file_name, 'wb') as f:\n",
        "    f.write(np.arange(value, value+4, dtype=np.int32))\n",
        "    \n",
        "bin_filenames = []\n",
        "for i in range(3):\n",
        "  file_name = 'binary_file_%d'% i\n",
        "  create_bin_file(file_name, i)\n",
        "  bin_filenames.append(file_name)\n",
        "\n",
        "# first file: 총 4바이트씩 16바이트\n",
        "# 0 1 2 3\n",
        "\n",
        "# second file:\n",
        "# 1 2 3 4\n",
        "\n",
        "# third file:\n",
        "# 2 3 4 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0rBvsoONqBSl"
      },
      "source": [
        "### FixedLengthRecordDataset : each fixed-length slice of bytes is a dataset element.\n",
        "\n",
        "In this example, each data instance is a 8-byte integer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "48JvZOGtqEwY",
        "colab": {},
        "outputId": "951e60e8-bfc6-45bf-8529-ca0b06beb4ec"
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # create a Dataset that contains slices (size: 8 bytes) of the files\n",
        "  ds = tf.data.FixedLengthRecordDataset(bin_filenames, 8)\n",
        "  # or equivalently,\n",
        "  # ds = tf.data.Dataset.from_tensor_slices(bin_filenames)\n",
        "  # ds = ds.apply(lambda filename: tf.data.FixedLengthRecordDataset(filename, 8))\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  # contains raw byte strings\n",
        "  iterator = iterator.get_next()\n",
        "  # convert 8 bytes into int32 => two int32 value per each element\n",
        "  to_int = tf.io.decode_raw(iterator, 'int32')\n",
        "\n",
        "iterate_and_print(to_int, graph) #16바이트니 8바이트씩 나옴"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: [0 1]\n",
            "step 1, data: [2 3]\n",
            "step 2, data: [1 2]\n",
            "step 3, data: [3 4]\n",
            "step 4, data: [2 3]\n",
            "step 5, data: [4 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L5FwnpyyneZQ"
      },
      "source": [
        "### Create dummy text files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BdSZO50jlSbB",
        "colab": {}
      },
      "source": [
        "def create_text_file(file_name, index):\n",
        "  with open(file_name, 'w') as f:\n",
        "    f.write('Hello_%d\\n' % index)\n",
        "    f.write('TensorFlow_%d\\n' % index)\n",
        "\n",
        "text_filenames = []\n",
        "for i in range(3):\n",
        "  file_name = 'text_file_%d'% i\n",
        "  create_text_file(file_name, i)\n",
        "  text_filenames.append(file_name)\n",
        "\n",
        "# first file:\n",
        "# Hello_0\n",
        "# TensorFlow_0\n",
        "\n",
        "# second file:\n",
        "# Hello_1\n",
        "# TensorFlow_1\n",
        "\n",
        "# third file:\n",
        "# Hello_2\n",
        "# TensorFlow_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jv2Vc8gt9WbO"
      },
      "source": [
        "### TextLineDataset : each text line is a dataset element.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hYY0TdipmvoU",
        "colab": {},
        "outputId": "e9a10955-1647-4d30-abf4-d24755217384"
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # create a Dataset that contains each line of the text files\n",
        "  ds = tf.data.TextLineDataset(text_filenames)\n",
        "  # or equivalently,\n",
        "  # ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
        "  # ds = ds.apply(lambda filename: tf.data.TextLineDataset(filename))\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'TensorFlow_0'\n",
            "step 2, data: b'Hello_1'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S8eqeLjir-ar"
      },
      "source": [
        "## Transform dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GinK40Kp5jQA"
      },
      "source": [
        "**ds.shffule(buffer_size)**\n",
        "\n",
        "shuffle: shuffle data instances randomly. buffer size represents the number of data instances to be sampled.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CzImNht48XKW"
      },
      "source": [
        "`ds.shuffle` with N > 1 can pick data instances randomly from the buffer containing N instances. The code snippet below shows that we always do not get the 5th or 6th element of the dataset (Hello_2 or TensorFlow_2) at step 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b3oaSOHVsGLc",
        "colab": {},
        "outputId": "5b1b0dbc-11dd-4159-c793-84aaa2301eb1"
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.TextLineDataset(text_filenames) \n",
        "  # shuffle the dataset using buffer size 4\n",
        "  ds = ds.shuffle(4) # 랜덤하게 버퍼중에 하나를 뽑음. 전체중에 일부만 4개보다 적게보는... 4개 이하 초과는 아님\n",
        "#가령, 처음은 1,2,3,4들어가게 되고 3이 없어지면 그 다음은 1,2,4,5중에 하나\n",
        "#stochestic gradiant에서 하나씩 받게 되는 경우가 있으므로..\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'TensorFlow_1'\n",
            "step 2, data: b'Hello_1'\n",
            "step 3, data: b'TensorFlow_2'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_0'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bP-MH37I8s8z"
      },
      "source": [
        "`ds.shuffle` with N == 1 has no shuffling effect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c8Q9W2pG6nxF",
        "colab": {}
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.TextLineDataset(text_filenames)\n",
        "  # shuffle the dataset using buffer size 1\n",
        "  ds = ds.shuffle(1)\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9i-rJwnE6Vdj"
      },
      "source": [
        "**ds.repeat(count)**\n",
        "\n",
        "Repeat the data instances count times. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WM9xpfjr72Qo"
      },
      "source": [
        "Without `ds.repeat()`, an error is raised after reading all the data from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUzyWsUt6WQ_",
        "colab": {}
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.TextLineDataset(text_filenames)\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph, count=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z_CwPo5I76G9"
      },
      "source": [
        "`ds.repeat(count)` repeats iterating the dataset `count` times. If we do not pass `count` argument, we can iterate forever."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FzqJY23X8Hg0",
        "colab": {}
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.TextLineDataset(text_filenames)\n",
        "  # repeat twice\n",
        "  ds = ds.repeat(2)\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph, count=12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0yM9UYyhdamR"
      },
      "source": [
        "Another common pattern is to use try-except clause to detect the end of epoch. Once we finisn an epoch, we re-initialize the iterator to start from the beginning again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NyHGXUxadamS",
        "colab": {}
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.TextLineDataset(text_filenames)\n",
        "\n",
        "  # create an empty iterator with only type and shape information\n",
        "  iterator = tf.data.Iterator.from_structure(tf.compat.v1.data.get_output_types(ds), tf.compat.v1.data.get_output_shapes(ds))\n",
        "\n",
        "  # create an operation for initializing the iterator with the dataset\n",
        "  init_iterator = iterator.make_initializer(ds)\n",
        "\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "epoch = 0\n",
        "step = 0\n",
        "with tf.Session(graph=graph) as sess:\n",
        "  # initialize the iterator\n",
        "  sess.run(init_iterator)\n",
        "  \n",
        "  while True:\n",
        "    # repeat until we detect an error\n",
        "    try:\n",
        "      v = sess.run(iterator)\n",
        "      print('step %d, data: %s' % (step, v))\n",
        "      step += 1\n",
        "    # iterator raises tf.errors.OutOfRangeError once we finish an epoch\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      print('Finished epoch', epoch)\n",
        "      epoch += 1\n",
        "      # if we are done with 2 epochs, break\n",
        "      if epoch >= 2:\n",
        "        break\n",
        "      # otherwise, re-initialize the iterator\n",
        "      sess.run(init_iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "71CqxItI4_Rk"
      },
      "source": [
        "**ds.batch(batch_size)**\n",
        "\n",
        "Combines elements of this dataset into batches. `batch_size` represents the number of data instances to combine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8stEmpyxvT18",
        "colab": {}
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.TextLineDataset(text_filenames) \n",
        "  # batch elements using batch_size 3\n",
        "  ds = ds.batch(3)\n",
        "  ds = ds.repeat(3)\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "meX0yDAd4KCD"
      },
      "source": [
        "**ds.map(fn)**\n",
        "\n",
        "Apply `fn` to each element of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zITLEwgys7l8",
        "colab": {}
      },
      "source": [
        "# split the `data` tensor into 3 pieces and concatenate the pieces by inserting '+' between them\n",
        "def split_join(data):\n",
        "  data = tf.split(data, 3)\n",
        "  return tf.strings.join(data, '+')\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.TextLineDataset(text_filenames)\n",
        "  ds = ds.batch(3)\n",
        "  ds = ds.repeat(3)\n",
        "  ds = ds.map(split_join)\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gXqUUZeo5a9E"
      },
      "source": [
        "##  Speed up Dataset processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WplDJMqQ59im"
      },
      "source": [
        "\n",
        "**ds.interleave(map_func, cycle_length)**\n",
        "\n",
        "map_func : map function to apply to each data instance\n",
        "\n",
        "cycle_length : the number of data instances to process concurrently"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "elNSpQlF9Kkd"
      },
      "source": [
        "We can use this feature to read and process multiple files concurrently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dlIOImd6KHHr",
        "colab": {}
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
        "  # consume the first two files in concurrently, and then the third file \n",
        "  ds = ds.interleave(lambda filename: tf.data.TextLineDataset(filename),\n",
        "                     cycle_length=2)\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mhp0R08AKxpy"
      },
      "source": [
        "**ds.prefetch(buffer_size)** \n",
        "\n",
        "prefetch elements from a dataset. buffer size represents the maximum buffer size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AWM2PH6SLHrb",
        "colab": {}
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
        "  ds = ds.interleave(lambda filename: tf.data.TextLineDataset(filename),\n",
        "                     cycle_length=3)\n",
        "  ds = ds.prefetch(3)\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fEs3qEJszYgu"
      },
      "source": [
        "## Quiz 4\n",
        "Create a dataset following the instructions.\n",
        "\n",
        "1. Create a textline dataset using files named `ex_filenames`. \n",
        "2. Shuffle the dataset with buffer size 15.\n",
        "3. Repeat the dataset for 2 epochs.\n",
        "4. Convert each data instance using the `cast` function defined below.\n",
        "5. Make the data instances as a batch (batch size = 3).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C_TOoWhx1O2S",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "def create_text_file(index):\n",
        "  with open('ex_file_%d'%index, 'w') as f:\n",
        "    for i in range(3):\n",
        "      f.write('%d.%d\\n' % (index, i))\n",
        "    \n",
        "ex_filenames = []\n",
        "for i in range(5):\n",
        "  create_text_file(i)\n",
        "  ex_filenames.append('ex_file_%d'% i)\n",
        "\n",
        "def cast(data):\n",
        "  data = tf.strings.to_number(data, out_type=tf.float32)\n",
        "  return data\n",
        "\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ############# Write here. ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ##########################################\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph, count=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z8oia-fDJ4cx"
      },
      "source": [
        "# 3. Logistic Regression\n",
        "\n",
        "Now, we can build a logistic regression example on TensorFlow by following the steps below.\n",
        "\n",
        "**1. Read MNIST data using the dataset API**\n",
        "\n",
        "**2. Create weights and biases**\n",
        "\n",
        "**3. Build a logistic regression model**\n",
        "\n",
        "**4. Define a loss function**\n",
        "\n",
        "**5. Define a training op**\n",
        "\n",
        "**6. Train and calculate accuracy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WYPbCvJxL8fH"
      },
      "source": [
        "## Read MNIST data using the dataset API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "86XRt-c9damj",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import gzip\n",
        "import os\n",
        "import sys\n",
        "import urllib\n",
        "\n",
        "try:\n",
        "    from urllib.error import URLError\n",
        "    from urllib.request import urlretrieve\n",
        "except ImportError:\n",
        "    from urllib2 import URLError\n",
        "    from urllib import urlretrieve\n",
        "\n",
        "RESOURCES = [\n",
        "    'train-images-idx3-ubyte.gz',\n",
        "    'train-labels-idx1-ubyte.gz',\n",
        "    't10k-images-idx3-ubyte.gz',\n",
        "    't10k-labels-idx1-ubyte.gz',\n",
        "]\n",
        "\n",
        "\n",
        "def report_download_progress(chunk_number, chunk_size, file_size):\n",
        "    if file_size != -1:\n",
        "        percent = min(1, (chunk_number * chunk_size) / file_size)\n",
        "        bar = '#' * int(64 * percent)\n",
        "        sys.stdout.write('\\r0% |{:<64}| {}%'.format(bar, int(percent * 100)))\n",
        "\n",
        "\n",
        "def download(destination_path, url, quiet):\n",
        "    if os.path.exists(destination_path):\n",
        "        if not quiet:\n",
        "            print('{} already exists, skipping ...'.format(destination_path))\n",
        "    else:\n",
        "        print('Downloading {} ...'.format(url))\n",
        "        try:\n",
        "            hook = None if quiet else report_download_progress\n",
        "            urlretrieve(url, destination_path, reporthook=hook)\n",
        "        except URLError:\n",
        "            raise RuntimeError('Error downloading resource!')\n",
        "        finally:\n",
        "            if not quiet:\n",
        "                # Just a newline.\n",
        "                print()\n",
        "\n",
        "\n",
        "def unzip(zipped_path, quiet):\n",
        "    unzipped_path = os.path.splitext(zipped_path)[0]\n",
        "    if os.path.exists(unzipped_path):\n",
        "        if not quiet:\n",
        "            print('{} already exists, skipping ... '.format(unzipped_path))\n",
        "        return\n",
        "    with gzip.open(zipped_path, 'rb') as zipped_file:\n",
        "        with open(unzipped_path, 'wb') as unzipped_file:\n",
        "            unzipped_file.write(zipped_file.read())\n",
        "            if not quiet:\n",
        "                print('Unzipped {}!'.format(zipped_path))\n",
        "\n",
        "\n",
        "mnist_dir = 'data/mnist'\n",
        "if not os.path.exists(mnist_dir):\n",
        "    os.makedirs(mnist_dir)\n",
        "\n",
        "\n",
        "for resource in RESOURCES:\n",
        "    path = os.path.join(mnist_dir, resource)\n",
        "    url = 'http://yann.lecun.com/exdb/mnist/{}'.format(resource)\n",
        "    download(path, url, False)\n",
        "    unzip(path, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "m_Pt_YweMFTP",
        "colab": {}
      },
      "source": [
        "import struct\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Parse each MNIST data instance as images and labels\n",
        "def parse_data(path, dataset, flatten):\n",
        "    if dataset != 'train' and dataset != 't10k':\n",
        "        raise NameError('dataset must be train or t10k')\n",
        "\n",
        "    label_file = os.path.join(path, dataset + '-labels-idx1-ubyte')\n",
        "    with open(label_file, 'rb') as file:\n",
        "        _, num = struct.unpack(\">II\", file.read(8))\n",
        "        labels = np.fromfile(file, dtype=np.int8) #int8\n",
        "        new_labels = np.zeros((num, 10))\n",
        "        new_labels[np.arange(num), labels] = 1\n",
        "    \n",
        "    img_file = os.path.join(path, dataset + '-images-idx3-ubyte')\n",
        "    with open(img_file, 'rb') as file:\n",
        "        _, num, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
        "        imgs = np.fromfile(file, dtype=np.uint8).reshape(num, rows, cols) #uint8\n",
        "        imgs = imgs.astype(np.float32) / 255.0\n",
        "        if flatten:\n",
        "            imgs = imgs.reshape([num, -1])\n",
        "\n",
        "    return imgs, new_labels\n",
        "  \n",
        "# Read in the mnist dataset, given that the data is stored in path\n",
        "# Return two tuples of numpy arrays\n",
        "# ((train_imgs, train_labels), (test_imgs, test_labels))\n",
        "def read_mnist(path, flatten=True):\n",
        "    train = parse_data(path, 'train', flatten)\n",
        "    test = parse_data(path, 't10k', flatten)\n",
        "    return train, test\n",
        "\n",
        "mnist_dir = \"data/mnist\"\n",
        "train, test = read_mnist(mnist_dir, flatten=True)\n",
        "\n",
        "print(train[0].shape) # image\n",
        "print(train[1].shape) # label\n",
        "print(test[0].shape) # image\n",
        "print(test[1].shape) # label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lq_2ypB720KU"
      },
      "source": [
        "### Build an input pipeline using the dataset API\n",
        "\n",
        "Create datasets from numpy arrays that contain MNIST data.\n",
        "Then, batch them using batch size of 128."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N1-_jKWwNGyC",
        "colab": {}
      },
      "source": [
        "train, test = read_mnist(mnist_dir, flatten=True)\n",
        "batch_size = 128\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  train_data = tf.data.Dataset.from_tensor_slices(train)\n",
        "  train_data = train_data.shuffle(10000)\n",
        "  train_data = train_data.batch(batch_size)\n",
        "\n",
        "  test_data = tf.data.Dataset.from_tensor_slices(test)\n",
        "  test_data = test_data.batch(batch_size)\n",
        "\n",
        "\n",
        "\n",
        "  # create an empty iterator with only type and shape information\n",
        "  iterator = tf.data.Iterator.from_structure(tf.compat.v1.data.get_output_types(train_data),\n",
        "                                             tf.compat.v1.data.get_output_shapes(train_data))\n",
        "  image, label = iterator.get_next()\n",
        "\n",
        "  # create an operation for initializing the iterator with the train or test dataset\n",
        "  train_init = iterator.make_initializer(train_data)\n",
        "  test_init = iterator.make_initializer(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8H5Yn3pxNVOH"
      },
      "source": [
        "## Build a logistic regression model\n",
        "\n",
        "`w` is initialized to random normal distribution variables with mean 0 and standard deviation 0.01.\n",
        "`b` is initialized to 0's.\n",
        "The shape of `w` depends on the dimensions of `image` and `label` so that `label = tf.matmul(image, w)`.\n",
        "The shape of `b` depends on `label`.\n",
        "The cross entropy of softmax of logits is our loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PKOmF9JfNZLc",
        "colab": {}
      },
      "source": [
        "with graph.as_default():\n",
        "  w = tf.get_variable(name='weights_dataset', shape=(784, 10), initializer=tf.random_normal_initializer(0, 0.01))\n",
        "  b = tf.get_variable(name='bias_dataset', shape=(1, 10), initializer=tf.zeros_initializer())\n",
        "  logits = tf.matmul(image, w) + b\n",
        "  loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=label)\n",
        "  loss = tf.reduce_mean(loss) # average over all the examples in the batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nZDF4B7jN5r9"
      },
      "source": [
        "## Define a training op\n",
        "\n",
        "We'll use an Adam optimizer with a learning rate of 0.001 to minimize loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hGPK-eWzN4yc",
        "colab": {}
      },
      "source": [
        "with graph.as_default():\n",
        "  optimizer = tf.train.AdamOptimizer(0.001)\n",
        "  train_op = optimizer.minimize(loss)\n",
        "  preds = tf.nn.softmax(logits)\n",
        "  num_corrects = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\n",
        "  num_corrects = tf.reduce_sum(tf.cast(num_corrects, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0BjGaGU4OM_O"
      },
      "source": [
        "## Train and calculate accuracy\n",
        "\n",
        "Finally, we train the model and use the test set to calculate the accuracy of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mfLm2JTjOOyy",
        "colab": {}
      },
      "source": [
        "with tf.Session(graph=graph) as sess:  \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # train the model for 10 epochs\n",
        "    for i in range(10):\n",
        "        sess.run(train_init) # initialize the iterator using training set\n",
        "        total_loss = 0\n",
        "        n_batches = 0\n",
        "        try:\n",
        "            while True:\n",
        "                _, l = sess.run([train_op, loss])\n",
        "                total_loss += l\n",
        "                n_batches += 1\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "        print('Epoch {0}: {1}'.format(i, total_loss/n_batches))   \n",
        "\n",
        "        # test the model\n",
        "        sess.run(test_init) # initialize the iterator using test set\n",
        "        total_num_corrects = 0\n",
        "        try:\n",
        "            while True: \n",
        "                total_num_corrects += sess.run(num_corrects)\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "        print('Accuracy {0}'.format(total_num_corrects/10000))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}