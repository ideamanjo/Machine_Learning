{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS2_191022_SparkRDD_SQL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ideamanjo/Machine_Learning/blob/master/DS2_191022_SparkRDD_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9JzWDhUXkh6",
        "colab_type": "text"
      },
      "source": [
        "Copyright (C) 2019 Seoul National University\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "## Colab 101\n",
        "\n",
        "Colab is a free Jupyter notebook environment by Google Research. Unlike AWS cluster (which is charged every hour it is up and running), you can run experiments on your own environment.\n",
        "\n",
        "## Colab Spark Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGKLcJu7HoB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless\n",
        "!pip3 install -q findspark\n",
        "!curl -OL http://mirror.navercorp.com/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar -xvzf spark-2.4.4-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSL4Z8LJHp4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-Pvn8rkXuHD",
        "colab_type": "text"
      },
      "source": [
        "## Wikipedia dataset sample\n",
        "\n",
        "This time we're not using HDFS to load the data. Sample data are loaded by Python code directly.\n",
        "\n",
        "The data has four fields: project, title, pageview count and size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTYHQ4OCU4LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wikipedia_data_sample = [\"commons.m File:Gemblong.JPG 1 9717\"\n",
        ",\"pl Beata_Tyszkiewicz 10 207378\"\n",
        ",\"en Special:RecentChangesLinked/Roswell_(TV_series) 1 14617\"\n",
        ",\"de Grafische_Benutzeroberfl%C3%A4che 1 22549\"\n",
        ",\"en Simeon_I_of_Bulgaria 5 385793\"\n",
        ",\"en Rainbow_Six_(novel) 8 122792\"\n",
        ",\"es Pediatr%C3%ADa 5 73598\"\n",
        ",\"sv Ett_uts%C3%B6kt_universum 1 9499\"\n",
        ",\"en Video_game_content_rating_system 4 112324\"\n",
        ",\"es Yuno_Gasai 2 55260\"\n",
        ",\"en File:Georg_Wilhelm_Friedrich_Hegel00.jpg 1 43395\"\n",
        ",\"en Anestia_ombrophanes 1 8881\"\n",
        ",\"et Seitse 2 84874\"\n",
        ",\"en And_I_Am_Telling_You_I%27m_Not_Going 4 85690\"\n",
        ",\"he %D7%A4%D7%A8%D7%93%D7%99%D7%92%D7%9E%D7%94 1 13887\"\n",
        ",\"zh File:Pictogram_voting_keep-green.svg 1 15106\"\n",
        ",\"sv Special:Senaste_relaterade_%C3%A4ndringar/Homestead,_Florida 1 7677\"\n",
        ",\"pt Categoria:Ambientes_de_desenvolvimento_integrado_livres 1 8151\"\n",
        ",\"de.voy Plattensee 1 43748\"\n",
        ",\"en Independent_Chip_Model 1 8938\"\n",
        ",\"en Category:Toronto_Toros_players 2 0\"\n",
        ",\"en Special:Export/Helsinki_Accords 1 19899\"\n",
        ",\"xh Special:Contributions/Kpeterzell 1 5883\"\n",
        ",\"nl 4_mei 1 0\"\n",
        ",\"no Carlos_Keller_Rueff 5 87075\"\n",
        ",\"en Special:Contributions/2.31.218.202 1 7402\"\n",
        ",\"es Placa_Yangtze 1 10329\"\n",
        ",\"de Datei:BSicon_uhKBHFe.svg 1 9786\"\n",
        ",\"en Randolph_County,_Alabama 1 21431\"\n",
        ",\"es S%C3%A9neca 3 70494\"\n",
        ",\"en Tu_Bishvat 3 56438\"\n",
        ",\"cs Radiohead 1 14325\"\n",
        ",\"es Naturaleza_sangre 1 9286\"\n",
        ",\"en Anatolia_(disambiguation) 1 7980\"\n",
        ",\"pt Queima_de_suti%C3%A3s 1 8982\"\n",
        ",\"pt Titanoboa_cerrejonensis 5 64540\"\n",
        ",\"commons.m Category:People_of_Ireland 1 19278\"\n",
        ",\"fi Matti_Inkinen 1 10138\"\n",
        ",\"ja %E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB:Esfahan_(Iran)_Emam_Mosque.JPG 1 33168\"\n",
        ",\"en Psicobloc 1 12739\"\n",
        ",\"en Macael,_Spain 1 12658\"\n",
        ",\"fa %DA%A9%D9%87%D8%AA%D9%88%DB%8C%D9%87 1 22855\"\n",
        ",\"fr Sp%C3%A9cial:Pages_li%C3%A9es/Fichier:Wiki-ezokuroten5.jpg 1 21955\"\n",
        ",\"nl Overleg_gebruiker:82.171.157.232 1 0\"\n",
        ",\"en Thomas_%26_Mack_Center 2 41010\"\n",
        ",\"en Warren_Beatty 49 2631986\"\n",
        ",\"uz Auberville 1 11401\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG9kePIjX6A5",
        "colab_type": "text"
      },
      "source": [
        "## Spark RDD Transforms and Actions\n",
        "\n",
        "RDDs support two types of operations.\n",
        "\n",
        "**Transformations** create a new dataset from an existing one\n",
        "\n",
        "**Actions** return a value to the driver program after running a computation on the dataset.\n",
        "\n",
        "From now, we'll try several Spark RDD transforms using the sample wikipedia dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n8go8ojHp_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# SparkSession is an entry point to programming Spark with the Dataset and DataFrame API\n",
        "# SparkContext represents a connection to a Spark cluster, which can be used to create RDD and broadcast variables on the cluster.\n",
        "\n",
        "ss = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = ss.sparkContext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZx1MpvJcdI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parallelize the data and split into columns\n",
        "lines = sc.parallelize(wikipedia_data_sample, 10)\n",
        "columns = lines.map(lambda line: tuple(line.split(\" \")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPcVG0I33j4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Element-Wise Transformation: Map Transform\n",
        "\n",
        "# Create (project, count) tuples(Be mindful of 'long()'!)\n",
        "project_count_tuples = columns.map(lambda column: (column[0], long(column[2])))\n",
        "project_count_tuples.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SADcMkDj3OVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Element-Wise Transformation: Filter Transform\n",
        "\n",
        "# Filter project containing name 'de'\n",
        "project_de_filtered = project_count_tuples.filter(lambda t: 'de' in t[0])\n",
        "project_de_filtered.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ficWLfQesGfe",
        "colab_type": "text"
      },
      "source": [
        "## Quiz\n",
        "Sample wikipedia data에서 project 의 count column 값이 5 이상인 경우만 filter 하시오.\n",
        "- 결과값: project, count 로 구성된 tuple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK9-KpetsHlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code here!\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwCCbynGn3tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Element-Wise Transformation: sortByKey Transform\n",
        "\n",
        "# Sort key-value tuples by key in ascending order\n",
        "project_sorted = project_count_tuples.sortByKey()\n",
        "project_sorted.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMI4h6a-q4yX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# And in descending order\n",
        "project_sorted_desc = project_count_tuples.sortByKey(ascending=False)\n",
        "project_sorted_desc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhZ3zYpqWiy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transformations on one Pair RDD: ReduceByKey Transform\n",
        "\n",
        "# Compute the sum of pageview counts per project\n",
        "project_sum_tuples = project_count_tuples.reduceByKey(lambda leftValue, rightValue: leftValue + rightValue) \n",
        "project_sum_tuples.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oblQYyRzsRfm",
        "colab_type": "text"
      },
      "source": [
        "## Quiz\n",
        "Sample wikipedia data에서 project 별로 count column 값을 곱하고 project가 'en'이 아닌 경우만 filter한 후, count 가 큰 순서대로 정렬하시오.\n",
        "\n",
        "- 결과값: project, count 로 구성된 tuple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym_-NoQttBfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code here!\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVcZtTxoJvGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transformations on two Pair RDDs: Join Transform\n",
        "\n",
        "# Declare another two sample data\n",
        "wikipedia_sample_singer = [\"en Steve_Jobs 49 2631986\"\n",
        ",\"en WoodKid 1 12739\"\n",
        ",\"en Honne 100 12658\"\n",
        ",\"fa Eminem 1 22855\"\n",
        ",\"en Sia 49 2631986\"]\n",
        "\n",
        "singer_to_ranking = [\"WoodKid 1\"\n",
        ",\"Honne 2\"\n",
        ",\"Eminem 3\"\n",
        ",\"Sia 4\"]\n",
        "\n",
        "# Parallelize the data and split into columns\n",
        "lines2 = sc.parallelize(wikipedia_sample_singer, 5)\n",
        "lines3 = sc.parallelize(singer_to_ranking, 4)\n",
        "\n",
        "wikipedia_sample_singer_tuples = lines2.map(lambda line: tuple(line.split(\" \")))\n",
        "singer_to_ranking_tuples = lines3.map(lambda line: tuple(line.split(\" \")))\n",
        "\n",
        "# Create (title, count) tuples and join via title name.\n",
        "title_count_tuples = wikipedia_sample_singer_tuples.map(lambda column: (column[1], long(column[2])))\n",
        "\n",
        "title_count_tuples.join(singer_to_ranking_tuples).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALQ1yxKUw7ug",
        "colab_type": "text"
      },
      "source": [
        "## Quiz\n",
        "Sample wikipedia data 와 아래의 project_to_projectid 를 이용하여 project 로 join 하시오.\n",
        "\n",
        "- 결과값: project, title 로 구성된 tuple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96lAqesrw6iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declare another sample data\n",
        "project_to_projectid = [\"en 1\"\n",
        ",\"fr 2\"\n",
        ",\"de 3\"\n",
        ",\"es 4\"]\n",
        "\n",
        "# Parallelize the data and split into columns\n",
        "\n",
        "# Create (project, title) tuples and join via project name.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgEM6fA2YaPz",
        "colab_type": "text"
      },
      "source": [
        "## SparkSQL\n",
        "\n",
        "Let's learn how to create SQL table by Spark DataFrame and execute SQL queries using Spark!\n",
        "We'll use Wikipedia data above to create the table, and try some SQL operations using its four colums - project, title, count, size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z650qgjHTFLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a Spark DataFrame from wikipedia_data_sample (equivalent of an 'SQL table' in Spark)\n",
        "df = ss.createDataFrame(columns, ['project', 'title', 'count', 'size'])\n",
        "\n",
        "# Create a table view called \"WikipediaTable\"\n",
        "df.createOrReplaceTempView(\"WikipediaTable\")\n",
        "\n",
        "df.show(df.count())\n",
        "\n",
        "# Run an SQL query that selects project equals to 'en' with count greater than or equal to 5\n",
        "selected = ss.sql(\"SELECT project,title FROM WikipediaTable WHERE (project = 'en' AND count >= 5)\")\n",
        "\n",
        "# Print the results in this console (top 20 results will be shown)\n",
        "selected.show(selected.count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw3J1GToUxl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run an SQL query that orders projects by the number of titles each project has\n",
        "selected = ss.sql(\"SELECT project AS p, COUNT(title) AS c FROM WikipediaTable \\\n",
        "GROUP BY project ORDER BY c DESC\")\n",
        "\n",
        "# Print the results in this console (top 20 results will be shown)\n",
        "selected.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_L1V1wTjDUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declare another two sample data\n",
        "wikipedia_sample_singer = [\"en Steve_Jobs 49 2631986\"\n",
        ",\"en WoodKid 1 12739\"\n",
        ",\"en Honne 100 12658\"\n",
        ",\"fa Eminem 1 22855\"\n",
        ",\"en Sia 49 2631986\"]\n",
        "\n",
        "singer_to_ranking = [\"WoodKid 1\"\n",
        ",\"Honne 2\"\n",
        ",\"Eminem 3\"\n",
        ",\"Sia 4\"]\n",
        "\n",
        "# Parallelize the data and split into columns\n",
        "lines2 = sc.parallelize(wikipedia_sample_singer, 5)\n",
        "lines3 = sc.parallelize(singer_to_ranking, 4)\n",
        "\n",
        "wikipedia_sample_singer_tuples = lines2.map(lambda line: tuple(line.split(\" \")))\n",
        "singer_to_ranking_tuples = lines3.map(lambda line: tuple(line.split(\" \")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN-tZTf_RasG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a Spark DataFrame from singer_to_ranking and wikipedia_sample_singer\n",
        "df = ss.createDataFrame(singer_to_ranking_tuples, ['title', 'ranking'])\n",
        "df1 = ss.createDataFrame(wikipedia_sample_singer_tuples, ['project', 'title', 'count', 'size'])\n",
        "\n",
        "# Create a table view of them, called \"RankingTable\" and \"SingerTable\"\n",
        "df.createOrReplaceTempView(\"RankingTable\")\n",
        "df1.createOrReplaceTempView(\"SingerTable\")\n",
        "\n",
        "# Run an SQL query that joins the two tables.\n",
        "# The result will show 'ranking' of RankingTable and 'title', 'count' of SingerTable.\n",
        "# Join will be performed on rows with common 'title' in both tables.\n",
        "df1.show()\n",
        "df.show()\n",
        "selected = ss.sql(\"SELECT ranking, SingerTable.title, count FROM RankingTable INNER JOIN SingerTable ON RankingTable.title = SingerTable.title ORDER BY RankingTable.ranking\")\n",
        "selected.show()\n",
        "#selected = ss.sql(\"SELECT RankingTable.ranking, SingerTable.title, SingerTable.count FROM SingerTable \\\n",
        "#                   INNER JOIN RankingTable ON RankingTable.title=SingerTable.title \\\n",
        "#                   ORDER BY RankingTable.ranking\")\n",
        "\n",
        "#selected.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzKCclPWYmG-",
        "colab_type": "text"
      },
      "source": [
        "## Spark SQL Quiz 1. \n",
        "'WikipediaTable'에서, 각 project 당 *count column 값의 총합이 10 이상인* (project, sum_of_count)를 구하시오\n",
        "- 결과값: project, sum_of_count 2개의 column 을 갖는 테이블"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp9V8qXNY5CS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Code here!\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLQZBBAiY_UN",
        "colab_type": "text"
      },
      "source": [
        "## Spark SQL Quiz 2.\n",
        "다음의 table을 'WikipediaTable'과 Join하여, grade가 'C'에 해당하는 project에 속하는 title들을 구하시오\n",
        "- 결과값: title 1개의 column 을 갖는 테이블"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm7cN8MuZNPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = ['project', 'grade']\n",
        "vals = [\n",
        "     ('en', 'C'),\n",
        "     ('he', 'A'),\n",
        "     ('zh', 'B'),    \n",
        "     ('no', 'A')\n",
        "]\n",
        "\n",
        "project_grade = ss.createDataFrame(vals, cols)\n",
        "project_grade.show()\n",
        "project_grade.createOrReplaceTempView(\"ProjectGradeTable\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EnpkIUQbTKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Code here!\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}